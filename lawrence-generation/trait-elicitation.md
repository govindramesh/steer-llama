I am investigating style transfer in language models. I want to develop a proof of concept. 

I have attached a recent paper by Anthropic about persona vectors. They took a trait, such as "evil", and generated a pair of system prompts to elicit and suppress this trait. Then, evaluation questions are generated to elicit this contrast. Then, a judge model evaluates whether or not the questions did indeed elicit the trait (or lack thereof). Then, taking the responses which have the highest trait expression, the authors extract the residual stream activations at every layer, and averaged across response tokens. Then, they computed the difference between the average activations from the elicitation and suppression prompts. They call this a persona vector. They verify that these steering vectors have causal effects on the model's sycophancy, for example, and system prompts encouraging a trait/persona lead to activation vectors that correspond with these personas. They say that they project the activation vector of the last prompt token onto the "evil" vector, for example. I don't really understand what the scalar value means because projections result in vectors, not scalars, so perhaps they meant the magnitude of the projection. 

I want to do something similar but for style. I have a model, Pi, that has a distinct style. I want to see if I can distill the style into a smaller model, like Llama-3.3-70B, using a steering vector. For example, Pi tends to avoid markdown formatting, use emojis, respond enthusiastically, and give short responses. 

First, we take a desired trait and generate a set of prompts (as done in persona vectors). These prompts will be broken into an extraction set and an evaluation set. We also ask the LLM generating these prompts to construct a per-item rubric for waht to look for. We then ask the Judge to use the rubric to see if the prompt managed to elicit the desired behavior.

Then we can take responses from Pi and teacher force them in Llama, and for each token, we collect the activations at each layer. Then we let Llama respond to the prompt without any teacher forcing. Persona vectors collects a simple 10 rollouts. They filter out the responses based on their trait expression scores. Then, we would average the activations (for each layer) across response tokens (or we can target specific tokens we believe are more likely to correspond with our desired behaviors, such as the emoji token for the emoji trait). This yields a candidate vector per layer, and we use the most informative layer by testing steering effectiveness while avoiding incoherence. In authors of persona vectors apply the same steering coefficient to all candidate vectosr and select the layer that elicits the highest trait expression score. Do other interpretability papers ever steer the model in multiple layers?