{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SteeringResult:\n",
    "    layer: int\n",
    "    alpha: float\n",
    "    prompt: str\n",
    "    response: str\n",
    "    coherence_score: int\n",
    "    trait_score: float\n",
    "    \n",
    "\n",
    "class CoherenceJudgment(BaseModel):\n",
    "    justification: str\n",
    "    answer: int  # 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_steering_vectors(vectors_path: str) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Load steering vectors from file.\"\"\"\n",
    "    return torch.load(vectors_path, map_location='cpu')\n",
    "\n",
    "\n",
    "def create_steering_hook(steering_vector: torch.Tensor, alpha: float):\n",
    "    \"\"\"Create a hook function that adds steering vector to residual stream.\"\"\"\n",
    "    def steering_hook(activations, hook):\n",
    "        # activations shape: (batch_size, seq_len, hidden_dim)\n",
    "        # Ensure steering vector matches activation device and dtype; operate out-of-place\n",
    "        vector = steering_vector.to(device=activations.device, dtype=activations.dtype)\n",
    "        return activations + alpha * vector\n",
    "    return steering_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hooked_generate(hooked_model: HookedTransformer, input_tokens, max_tokens, layer_name, hook_fn, temperature=0.7, do_sample=True, top_p=0.9, **kwargs):\n",
    "    \"\"\"Custom generation loop using hooked forward passes.\"\"\"\n",
    "    \n",
    "    # Start with the initial input tokens\n",
    "    generated_ids = input_tokens.clone()\n",
    "    \n",
    "    for _ in tqdm(range(max_tokens)):\n",
    "        # Run forward pass with hooks\n",
    "        logits = hooked_model.run_with_hooks(\n",
    "            generated_ids, \n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=[(layer_name, hook_fn)]\n",
    "        )\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        if do_sample and temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "        \n",
    "        # Get next token\n",
    "        if do_sample:\n",
    "            # Apply top-p (nucleus) sampling if specified\n",
    "            if top_p < 1.0:\n",
    "                # Sort logits and get cumulative probabilities\n",
    "                sorted_logits, sorted_indices = torch.sort(logits[0, -1], descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # Keep at least one token\n",
    "                sorted_indices_to_remove[0] = False\n",
    "                \n",
    "                # Set logits of removed tokens to -inf\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[0, -1, indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.softmax(logits[0, -1], dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "        else:\n",
    "            # Greedy sampling\n",
    "            next_token = logits[0, -1].argmax().unsqueeze(0)\n",
    "        \n",
    "        # Reshape to match expected dimensions\n",
    "        next_token = next_token.unsqueeze(0)\n",
    "\n",
    "        # Append the new token\n",
    "        # move everything to cuda:0\n",
    "        generated_ids = generated_ids.to(\"cuda:0\")\n",
    "        next_token = next_token.to(\"cuda:0\")\n",
    "        generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "        \n",
    "        # Check for EOS token (optional - depends on your tokenizer)\n",
    "        if hasattr(hooked_model.tokenizer, 'eos_token_id') and next_token.item() == hooked_model.tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_steered_response(\n",
    "    model: HookedTransformer,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    layer_name: str,\n",
    "    steering_vector: torch.Tensor,\n",
    "    alpha: float,\n",
    "    max_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    do_sample: bool = True,\n",
    "    top_p: float = 0.9\n",
    ") -> str:\n",
    "    \"\"\"Generate response with steering applied at specified layer.\"\"\"\n",
    "    \n",
    "    # Prepare input\n",
    "    try:\n",
    "        # Try to use chat template if available\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except (ValueError, AttributeError):\n",
    "        # Fallback to raw prompt if no chat template\n",
    "        print(\"Warning: No chat template available, using raw prompt\")\n",
    "        input_text = prompt\n",
    "    \n",
    "    # Tokenize\n",
    "    input_tokens = model.to_tokens(input_text)\n",
    "    \n",
    "    # Add steering hook\n",
    "    hook_fn = create_steering_hook(steering_vector, alpha)\n",
    "    \n",
    "    # Generate with steering using custom loop\n",
    "    with torch.no_grad():\n",
    "        generated_ids = hooked_generate(\n",
    "            model, \n",
    "            input_tokens, \n",
    "            max_tokens, \n",
    "            layer_name, \n",
    "            hook_fn,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            top_p=top_p\n",
    "        )\n",
    "    \n",
    "    # Decode response (skip original input)\n",
    "    response = tokenizer.decode(generated_ids[0][input_tokens.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coherence(response: str, prompt: str) -> int:\n",
    "    \"\"\"Evaluate response coherence using Qwen3 API.\"\"\"\n",
    "    \n",
    "    coherence_prompt = f\"\"\"You are evaluating whether a response is coherent and makes sense.\n",
    "\n",
    "Prompt: {prompt}\n",
    "Response: {response}\n",
    "\n",
    "A coherent response should:\n",
    "1. Be grammatically correct and well-structured\n",
    "2. Make logical sense and stay on topic\n",
    "3. Be appropriate for the given prompt\n",
    "4. Not contain contradictions or nonsensical statements\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "- justification: Brief explanation of your assessment\n",
    "- answer: 0 if incoherent, 1 if coherent\"\"\"\n",
    "\n",
    "    try:\n",
    "        response_obj = requests.post(\n",
    "            \"http://172.28.127.109:8010/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"qwen3-235b-reasoning\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": coherence_prompt}],\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 200\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response_obj.status_code == 200:\n",
    "            result = response_obj.json()\n",
    "            content = result['choices'][0]['message']['content']\n",
    "            \n",
    "            # Parse JSON response\n",
    "            judgment = CoherenceJudgment.model_validate_json(content)\n",
    "            return judgment.answer\n",
    "        else:\n",
    "            print(f\"Coherence API error: {response_obj.status_code}\")\n",
    "            return 1  # Default to coherent if API fails\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Coherence evaluation error: {e}\")\n",
    "        return 1  # Default to coherent if evaluation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trait_expression(response: str, prompt: str) -> float:\n",
    "    \"\"\"Evaluate emoji trait expression using the existing eval prompt.\"\"\"\n",
    "    \n",
    "    eval_prompt_template = \"\"\"You are evaluating whether the model's response displays the trait: emoji usage.\n",
    "The model tends to use emojis in its responses, particularly when the user uses emojis or expresses emotion, excitement, or casual tone. The use of emojis should feel natural and contextually appropriate, enhancing the expressiveness of the response.\n",
    "Prompt:\n",
    "[QUESTION START]\n",
    "{question}\n",
    "[QUESTION END]\n",
    "Model response:\n",
    "[ANSWER START]\n",
    "{answer}\n",
    "[ANSWER END]\n",
    "Focus only on whether the model exhibits this trait.\n",
    "Respond with:\n",
    "- REFUSAL if the model clearly refused to answer\n",
    "- A score from 0 (trait not present at all) to 100 (trait strongly present)\n",
    "Respond with a number between 0 and 100. Don't say anything else, just the number.\"\"\"\n",
    "    \n",
    "    eval_prompt = eval_prompt_template.format(question=prompt, answer=response)\n",
    "    \n",
    "    try:\n",
    "        response_obj = requests.post(\n",
    "            \"http://172.28.127.109:8010/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"qwen3-235b-reasoning\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 10\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response_obj.status_code == 200:\n",
    "            result = response_obj.json()\n",
    "            content = result['choices'][0]['message']['content'].strip()\n",
    "            \n",
    "            # Extract numeric score\n",
    "            if content == \"REFUSAL\":\n",
    "                return 0.0\n",
    "            else:\n",
    "                return float(content)\n",
    "        else:\n",
    "            print(f\"Trait API error: {response_obj.status_code}\")\n",
    "            return 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Trait evaluation error: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steering_experiment(\n",
    "    model: HookedTransformer,\n",
    "    tokenizer,\n",
    "    steering_vectors: Dict[str, torch.Tensor],\n",
    "    test_prompts: List[str],\n",
    "    alphas: List[float],\n",
    "    output_dir: str\n",
    ") -> List[SteeringResult]:\n",
    "    \"\"\"Run steering experiment across all layers and alpha values.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Get layer names (sorted)\n",
    "    layer_names = sorted(steering_vectors.keys())\n",
    "    \n",
    "    print(f\"Testing {len(layer_names)} layers × {len(alphas)} alphas × {len(test_prompts)} prompts = {len(layer_names) * len(alphas) * len(test_prompts)} total conditions\")\n",
    "    \n",
    "    for layer_name in layer_names:\n",
    "        layer_num = int(layer_name.split('.')[1])  # Extract number from \"blocks.X.hook_resid_post\"\n",
    "        steering_vector = steering_vectors[layer_name]\n",
    "        \n",
    "        print(f\"\\nTesting layer {layer_num} ({layer_name})...\")\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            print(f\"  Alpha = {alpha}\")\n",
    "            \n",
    "            for prompt in test_prompts:\n",
    "                print(f\"    Prompt: {prompt[:50]}...\")\n",
    "                \n",
    "                # Generate steered response\n",
    "                response = generate_steered_response(\n",
    "                    model, tokenizer, prompt, layer_name, steering_vector, alpha\n",
    "                )\n",
    "                \n",
    "                # Evaluate response\n",
    "                coherence = evaluate_coherence(response, prompt)\n",
    "                trait_score = evaluate_trait_expression(response, prompt) if coherence == 1 else 0.0\n",
    "                \n",
    "                result = SteeringResult(\n",
    "                    layer=layer_num,\n",
    "                    alpha=alpha,\n",
    "                    prompt=prompt,\n",
    "                    response=response,\n",
    "                    coherence_score=coherence,\n",
    "                    trait_score=trait_score\n",
    "                )\n",
    "                \n",
    "                results.append(result)\n",
    "                print(result)\n",
    "                \n",
    "                print(f\"      Coherence: {coherence}, Trait: {trait_score:.1f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results: List[SteeringResult], output_path: str):\n",
    "    \"\"\"Create visualization of steering results.\"\"\"\n",
    "    \n",
    "    # Filter to coherent responses only\n",
    "    coherent_results = [r for r in results if r.coherence_score == 1]\n",
    "    \n",
    "    if not coherent_results:\n",
    "        print(\"No coherent results to plot!\")\n",
    "        return\n",
    "    \n",
    "    # Group by layer and alpha, average trait scores\n",
    "    layer_alpha_scores = {}\n",
    "    for result in coherent_results:\n",
    "        key = (result.layer, result.alpha)\n",
    "        if key not in layer_alpha_scores:\n",
    "            layer_alpha_scores[key] = []\n",
    "        layer_alpha_scores[key].append(result.trait_score)\n",
    "    \n",
    "    # Average scores\n",
    "    plot_data = {}\n",
    "    for (layer, alpha), scores in layer_alpha_scores.items():\n",
    "        plot_data[(layer, alpha)] = np.mean(scores)\n",
    "    \n",
    "    # Get unique layers and alphas\n",
    "    layers = sorted(set(layer for layer, alpha in plot_data.keys()))\n",
    "    alphas = sorted(set(alpha for layer, alpha in plot_data.keys()))\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        layer_scores = []\n",
    "        for layer in layers:\n",
    "            score = plot_data.get((layer, alpha), 0.0)\n",
    "            layer_scores.append(score)\n",
    "        \n",
    "        plt.plot(layers, layer_scores, marker='o', label=f'α = {alpha}')\n",
    "    \n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Trait Expression Score (0-100)')\n",
    "    plt.title('Steering Effectiveness Across Layers')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot to {output_path}\")\n",
    "\n",
    "\n",
    "def save_results(results: List[SteeringResult], output_path: str):\n",
    "    \"\"\"Save detailed results to JSON.\"\"\"\n",
    "    results_data = []\n",
    "    for r in results:\n",
    "        results_data.append({\n",
    "            \"layer\": r.layer,\n",
    "            \"alpha\": r.alpha,\n",
    "            \"prompt\": r.prompt,\n",
    "            \"response\": r.response,\n",
    "            \"coherence_score\": r.coherence_score,\n",
    "            \"trait_score\": r.trait_score\n",
    "        })\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved detailed results to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [\"steer_model.py\", \n",
    "            \"--steering-vectors\", \"/mnt/vast/home/lawrence/steer-llama/lawrence-steer/emoji_steering_vectors.pt\", \n",
    "            \"--model-path\", \"/mnt/vast/share/inf2-training/models/open_source/llama-3.3-70B-Instruct\", \n",
    "            \"--test-prompts\", \"Do you think people text too much these days?\", \"Is it weird to talk to AI about feelings?\",\n",
    "            \"--alphas\", \"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\", \n",
    "            \"--output-dir\", \"results\"]\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run steering experiment\")\n",
    "parser.add_argument(\"--steering-vectors\", required=True, help=\"Path to steering vectors file\")\n",
    "parser.add_argument(\"--model-path\", required=True, help=\"Path to model\")\n",
    "parser.add_argument(\"--test-prompts\", nargs=\"+\", required=True, help=\"Test prompts\")\n",
    "parser.add_argument(\"--alphas\", nargs=\"+\", type=float, default=[0.5, 1.0, 1.5, 2.0, 2.5])\n",
    "parser.add_argument(\"--output-dir\", required=True, help=\"Output directory\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 356.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-70B-Instruct into HookedTransformer\n",
      "Loading steering vectors...\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "import os\n",
    "hf_token = os.getenv(\"HF_KEY\")\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n",
    "    print(f\"Using HF token for model access...\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    device=\"cuda\",\n",
    "    n_devices=8,\n",
    "    dtype=torch.bfloat16,\n",
    "    name_or_path=args.model_path,\n",
    "    move_to_device=True\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Load steering vectors\n",
    "print(\"Loading steering vectors...\")\n",
    "steering_vectors = load_steering_vectors(args.steering_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading steering vectors...\n",
      "Loaded steering vectors\n"
     ]
    }
   ],
   "source": [
    "# Load steering vectors\n",
    "print(\"Loading steering vectors...\")\n",
    "steering_vectors = load_steering_vectors(args.steering_vectors)\n",
    "print(\"Loaded steering vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running steering experiment...\n",
      "Testing 80 layers × 5 alphas × 2 prompts = 800 total conditions\n",
      "\n",
      "Testing layer 0 (blocks.0.hook_resid_post)...\n",
      "  Alpha = 0.5\n",
      "    Prompt: Do you think people text too much these days?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:46<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run experiment\n",
    "print(\"Running steering experiment...\")\n",
    "results = run_steering_experiment(\n",
    "    model, tokenizer, steering_vectors, args.test_prompts, args.alphas, args.output_dir\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_path = Path(args.output_dir) / \"results.json\"\n",
    "save_results(results, str(results_path))\n",
    "\n",
    "# Create plot\n",
    "plot_path = Path(args.output_dir) / \"steering_results.png\"\n",
    "plot_results(results, str(plot_path))\n",
    "\n",
    "# Print summary\n",
    "coherent_count = sum(1 for r in results if r.coherence_score == 1)\n",
    "total_count = len(results)\n",
    "print(f\"\\nSummary: {coherent_count}/{total_count} responses were coherent\")\n",
    "\n",
    "if coherent_count > 0:\n",
    "    coherent_results = [r for r in results if r.coherence_score == 1]\n",
    "    avg_trait_score = np.mean([r.trait_score for r in coherent_results])\n",
    "    print(f\"Average trait score (coherent responses): {avg_trait_score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "my_uv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
