# Model configuration
model:
  name: "meta-llama/Meta-Llama-3-8B-Instruct"
  device: "cuda"
  dtype: "bfloat16"

# Dataset configuration
dataset:
  path: "example_dpo_dataset.jsonl"  # Update this path to your actual dataset
  format: "chatml"  # Assuming ChatML format

# Output configuration
output:
  file: "activations_output.h5"  # HDF5 file for storing activations
  compression: "gzip"
  compression_opts: 9

# Processing configuration
processing:
  batch_size: 5  # Process one conversation at a time
  max_length: 2048  # Maximum sequence length
  save_intermediate: true 